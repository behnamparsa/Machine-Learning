{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE3tsGb1Ejv0",
        "outputId": "7aef5ee1-caa6-4b20-e582-b3761a3065e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'assistant', 'content': \"Yer lookin' fer me, eh? I be Captain Blackbeak Betty, the most feared and infamous pirate to ever sail the seven seas. Me and me crew o' scurvy dogs have been plunderin' and pillagin' fer years, and we ain't no strangers to a good fight. Me trusty cutlass by me side, me cunning wit, and me love o' a good treasure map be me greatest assets. So hoist the sails and set course fer adventure with ol' Blackbeak Betty at the helm!\"}\n"
          ]
        }
      ],
      "source": [
        "# Import the PyTorch library for tensor computations.\n",
        "import torch\n",
        "\n",
        "# Import the pipeline function from the transformers library for easy model usage.\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "# Your Hugging Face access token for accessing the model.\n",
        "access_token = \"\" # Use your own token\n",
        "\n",
        "# The ID of the pre-trained language model you want to use.\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Create a text generation pipeline using the specified model and parameters.\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",  # Task type: text generation\n",
        "    model=model_id,  # Model ID\n",
        "    torch_dtype=torch.bfloat16,  # Data type for computations (bfloat16 for efficiency)\n",
        "    device_map=\"auto\",  # Automatically determine device (GPU or CPU)\n",
        "    token=access_token,  # Your Hugging Face access token\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Define the conversation messages for the chatbot.\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a pirate chatbot who always responds in pirate speak!\",\n",
        "    },  # System message setting the chatbot's persona\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},  # User's message\n",
        "]\n",
        "\n",
        "\n",
        "# Generate text using the pipeline and the provided messages.\n",
        "outputs = pipe(\n",
        "    messages,  # Input messages\n",
        "    max_new_tokens=512,  # Maximum number of tokens to generate\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Print the last character of the generated text.\n",
        "print(outputs[0][\"generated_text\"][-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentiment(text):\n",
        "    \"\"\"\n",
        "    Classifies the sentiment of a given text using the Llama 3.2-1B Instruct model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be classified.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted sentiment, either \"positive\", \"negative\", or \"neutral\".\n",
        "    \"\"\"\n",
        "    # Define the conversation messages for the chatbot, including instructions for output format.\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are a sentiment classifier.\n",
        "            Respond only with the word \"positive\" or \"negative\".\n",
        "            Do not include any other text or explanations.\n",
        "            \"\"\",\n",
        "        },  # System message setting the chatbot's persona and desired output format.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Classify the sentiment of the following text: '{text}'\",\n",
        "        },  # User's message with the text to be classified.\n",
        "    ]\n",
        "\n",
        "    # Generate text using the pipeline and the provided messages.\n",
        "    outputs = pipe(messages, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Extract and process the sentiment from the generated text.\n",
        "    sentiment = outputs[0][\"generated_text\"][-1]['content'].strip().lower()\n",
        "\n",
        "    # Simple sentiment mapping (you can customize this)\n",
        "    if \"positive\" in sentiment:\n",
        "        return \"positive\"  # Return \"positive\" if the model output contains \"positive\".\n",
        "    elif \"negative\" in sentiment:\n",
        "        return \"negative\"  # Return \"negative\" if the model output contains \"negative\".\n",
        "    else:\n",
        "        return \"neutral\"   # Return \"neutral\" if neither \"positive\" nor \"negative\" is found.\n",
        "\n",
        "# Example usage\n",
        "text = \"This movie was amazing!\"\n",
        "sentiment = classify_sentiment(text)\n",
        "print(f\"Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "k-VM_hFSGTGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe4e403-0a6f-4cbd-b36e-223097d550eb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 1\n",
        "def summarize_text(text):\n",
        "    \"\"\"\n",
        "    Summarizes a given text using the Llama 3.2-1B Instruct model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be summarized.\n",
        "\n",
        "    Returns:\n",
        "        str: A concise summary of the input text.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "qchBlBaQuW5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 2\n",
        "def generate_character_names(num_names, race=\"elf\", gender=\"female\"):\n",
        "    \"\"\"\n",
        "    Generates fantasy character names using a loop.\n",
        "\n",
        "    Args:\n",
        "        num_names (int): The number of names to generate.\n",
        "        race (str, optional): The race of the characters (e.g., \"elf\", \"dwarf\", \"human\"). Defaults to \"elf\".\n",
        "        gender (str, optional): The gender of the characters (e.g., \"male\", \"female\"). Defaults to \"female\".\n",
        "    Output:\n",
        "          List[str]: A list of generated character names.\n",
        "    Note:\n",
        "    Make sure to prompt the model to output in the desired format.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "Jj1KTRX-uaBo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_character_names(5, race=\"elf\", gender=\"female\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DM0i6lFuvGs",
        "outputId": "aea42a36-ce4d-4e5b-ba77-72bcb69ce583"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Aethonielyn', 'Eirlys Althaea', 'Aethonielwynn', 'Aethoniel', 'Eiravynn']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "In1tDca8vizJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}