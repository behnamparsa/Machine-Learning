{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Train Dataset Rating Distribution:\n",
      " Rating\n",
      "5    2526\n",
      "4    2526\n",
      "3    2297\n",
      "2    1252\n",
      "1     674\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New Test Dataset Rating Distribution:\n",
      " Rating\n",
      "5    10605\n",
      "4     2551\n",
      "3      574\n",
      "2      313\n",
      "1      168\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(\"C:/GitHub/Machine-Learning/INF1279H/train_data.csv\")\n",
    "test_data = pd.read_csv(\"C:/GitHub/Machine-Learning/INF1279H/test_data.csv\")\n",
    "\n",
    "# Merge both datasets\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Ensure no missing values in the Rating column\n",
    "combined_data = combined_data.dropna(subset=[\"Rating\"])\n",
    "\n",
    "# Define storage for new train and test datasets\n",
    "train_balanced = pd.DataFrame()\n",
    "test_balanced = pd.DataFrame()\n",
    "\n",
    "# Define limits\n",
    "TEST_RATIO = 0.2  # Ensure at least 20% of each rating remains in test dataset\n",
    "\n",
    "# Process each rating category separately\n",
    "for rating, group in combined_data.groupby(\"Rating\"):\n",
    "    # Ensure at least 20% of records are in test dataset\n",
    "    test_size = max(int(len(group) * TEST_RATIO), 1)\n",
    "    test_subset = group.sample(n=test_size, random_state=42)\n",
    "    test_balanced = pd.concat([test_balanced, test_subset])\n",
    "\n",
    "    # Remaining records (not in test set)\n",
    "    remaining = group.drop(test_subset.index)\n",
    "\n",
    "    # Find the minimum number of records across all ratings in train\n",
    "    min_train_samples = min(len(remaining) for _, remaining in combined_data.groupby(\"Rating\"))\n",
    "\n",
    "    # Limit training samples for each rating to at most 3 times the minimum count\n",
    "    train_size = min(len(remaining), min_train_samples * 3)\n",
    "    \n",
    "    # Select training subset\n",
    "    train_subset = remaining.sample(n=train_size, random_state=42)\n",
    "    train_balanced = pd.concat([train_balanced, train_subset])\n",
    "\n",
    "    # Move any leftover records to test dataset\n",
    "    leftover_records = remaining.drop(train_subset.index)\n",
    "    test_balanced = pd.concat([test_balanced, leftover_records])\n",
    "\n",
    "# Save new train/test datasets\n",
    "train_balanced.to_csv(\"C:/GitHub/Machine-Learning/INF1279H/new_train_data.csv\", index=False)\n",
    "test_balanced.to_csv(\"C:/GitHub/Machine-Learning/INF1279H/new_test_data.csv\", index=False)\n",
    "\n",
    "# Print distribution check\n",
    "print(\"New Train Dataset Rating Distribution:\\n\", train_balanced[\"Rating\"].value_counts())\n",
    "print(\"\\nNew Test Dataset Rating Distribution:\\n\", test_balanced[\"Rating\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
